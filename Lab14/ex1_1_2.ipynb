import numpy as np
import pandas as pd
from scipy.stats import norm
from scipy.special import logsumexp
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('date_colesterol.csv')
t = data['Ore_Exercitii'].values
y = data['Colesterol'].values
n = len(y)

def initialize_params(K, t, y):
    np.random.seed(42)
    params = {}
    params['w'] = np.ones(K) / K
    params['alpha'] = np.random.randn(K) * 10 + np.mean(y)
    params['beta'] = np.random.randn(K) * 5
    params['gamma'] = np.random.randn(K) * 0.5
    params['sigma'] = np.ones(K) * np.std(y) * 0.5
    return params

def compute_mu(alpha, beta, gamma, t):
    return alpha + beta * t + gamma * t**2

def e_step(t, y, params, K):
    n = len(y)
    log_resp = np.zeros((n, K))

    for k in range(K):
        mu = compute_mu(params['alpha'][k], params['beta'][k], params['gamma'][k], t)
        log_resp[:, k] = np.log(params['w'][k] + 1e-10) + norm.logpdf(y, mu, params['sigma'][k])

    log_resp_norm = logsumexp(log_resp, axis=1, keepdims=True)
    log_resp = log_resp - log_resp_norm
    resp = np.exp(log_resp)

    return resp

def m_step(t, y, resp, K):
    n = len(y)
    params = {}

    N_k = resp.sum(axis=0)
    params['w'] = N_k / n

    params['alpha'] = np.zeros(K)
    params['beta'] = np.zeros(K)
    params['gamma'] = np.zeros(K)
    params['sigma'] = np.zeros(K)

    for k in range(K):
        t2 = t**2
        t3 = t**3
        t4 = t**4

        w_k = resp[:, k]
        sum_w = np.sum(w_k)

        sum_wt = np.sum(w_k * t)
        sum_wt2 = np.sum(w_k * t2)
        sum_wt3 = np.sum(w_k * t3)
        sum_wt4 = np.sum(w_k * t4)
        sum_wy = np.sum(w_k * y)
        sum_wty = np.sum(w_k * t * y)
        sum_wt2y = np.sum(w_k * t2 * y)

        A = np.array([
            [sum_w, sum_wt, sum_wt2],
            [sum_wt, sum_wt2, sum_wt3],
            [sum_wt2, sum_wt3, sum_wt4]
        ])
        b = np.array([sum_wy, sum_wty, sum_wt2y])

        try:
            coeffs = np.linalg.solve(A, b)
            params['alpha'][k] = coeffs[0]
            params['beta'][k] = coeffs[1]
            params['gamma'][k] = coeffs[2]
        except:
            params['alpha'][k] = np.sum(w_k * y) / sum_w
            params['beta'][k] = 0
            params['gamma'][k] = 0

        mu = compute_mu(params['alpha'][k], params['beta'][k], params['gamma'][k], t)
        params['sigma'][k] = np.sqrt(np.sum(w_k * (y - mu)**2) / sum_w)
        params['sigma'][k] = max(params['sigma'][k], 1e-3)

    return params

def compute_log_likelihood(t, y, params, K):
    n = len(y)
    log_like = 0

    for i in range(n):
        ll_i = []
        for k in range(K):
            mu = compute_mu(params['alpha'][k], params['beta'][k], params['gamma'][k], t[i])
            ll_i.append(np.log(params['w'][k] + 1e-10) + norm.logpdf(y[i], mu, params['sigma'][k]))
        log_like += logsumexp(ll_i)

    return log_like

def em_algorithm(t, y, K, max_iter=200, tol=1e-6):
    params = initialize_params(K, t, y)
    log_like_old = -np.inf

    for iteration in range(max_iter):
        resp = e_step(t, y, params, K)
        params = m_step(t, y, resp, K)
        log_like = compute_log_likelihood(t, y, params, K)

        if abs(log_like - log_like_old) < tol:
            break
        log_like_old = log_like

    return params, log_like

def compute_waic(t, y, params, K):
    n = len(y)
    lppd = 0
    p_waic = 0

    log_likes = np.zeros(n)
    log_likes_sq = np.zeros(n)

    for i in range(n):
        ll_i = []
        for k in range(K):
            mu = compute_mu(params['alpha'][k], params['beta'][k], params['gamma'][k], t[i])
            ll_i.append(np.log(params['w'][k] + 1e-10) + norm.logpdf(y[i], mu, params['sigma'][k]))

        log_like_i = logsumexp(ll_i)
        lppd += log_like_i

        likes = []
        for k in range(K):
            mu = compute_mu(params['alpha'][k], params['beta'][k], params['gamma'][k], t[i])
            likes.append(params['w'][k] * norm.pdf(y[i], mu, params['sigma'][k]))

        mean_like = np.mean(likes)
        var_like = np.var(likes)
        p_waic += var_like

    waic = -2 * (lppd - p_waic)
    return waic

def compute_loo(t, y, params, K):
    n = len(y)
    loo_sum = 0

    for i in range(n):
        ll_i = []
        for k in range(K):
            mu = compute_mu(params['alpha'][k], params['beta'][k], params['gamma'][k], t[i])
            ll_i.append(np.log(params['w'][k] + 1e-10) + norm.logpdf(y[i], mu, params['sigma'][k]))
        loo_sum += logsumexp(ll_i)

    loo = -2 * loo_sum
    return loo

# Exercitiul 1
print("Exercitiul 1: Estimarea greutatilor si coeficientilor de regresie\n")

results = {}
for K in [3, 4, 5]:
    print(f"K = {K} subpopulatii:")
    params, log_like = em_algorithm(t, y, K)
    results[K] = {'params': params, 'log_like': log_like}

    print(f"  Log-likelihood: {log_like:.2f}")
    print(f"  Greutati (w): {params['w']}")
    print(f"  Alpha: {params['alpha']}")
    print(f"  Beta: {params['beta']}")
    print(f"  Gamma: {params['gamma']}")
    print(f"  Sigma: {params['sigma']}")
    print()

# Exercitiul 2
print("\nExercitiul 2: Selectia numarului optim de subpopulatii\n")

criterias = {}
for K in [3, 4, 5]:
    params = results[K]['params']
    waic = compute_waic(t, y, params, K)
    loo = compute_loo(t, y, params, K)
    criterias[K] = {'WAIC': waic, 'LOO': loo}
    print(f"K = {K}:")
    print(f"  WAIC: {waic:.2f}")
    print(f"  LOO: {loo:.2f}")
    print()

best_k_waic = min(criterias.keys(), key=lambda k: criterias[k]['WAIC'])
best_k_loo = min(criterias.keys(), key=lambda k: criterias[k]['LOO'])

print("Concluzie:")
print(f"  Conform WAIC: K = {best_k_waic} subpopulatii (WAIC = {criterias[best_k_waic]['WAIC']:.2f})")
print(f"  Conform LOO: K = {best_k_loo} subpopulatii (LOO = {criterias[best_k_loo]['LOO']:.2f})")
print(f"\nNumarul optim de subpopulatii este K = {best_k_waic}")
